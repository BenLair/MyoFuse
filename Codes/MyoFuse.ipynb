{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91571ad7-fa8d-428c-95f4-83fe1aabaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT LIBRARIES ##\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tifffile import imwrite\n",
    "from aicsimageio import AICSImage\n",
    "from cellpose import models\n",
    "from skimage import img_as_float, exposure\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.measure import regionprops\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.color import label2rgb\n",
    "from skimage.segmentation import find_boundaries\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "if torch.cuda.is_available() is True:\n",
    "    try:\n",
    "        import cupy as cu\n",
    "        from cucim.skimage.morphology import dilation, disk\n",
    "        cuda = True\n",
    "    except ImportError:\n",
    "        from skimage.morphology import dilation, disk\n",
    "        cuda = False\n",
    "else:\n",
    "    from skimage.morphology import dilation, disk\n",
    "    cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8031639-6434-4496-91c0-13330929e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER YOUR VALUES ##\n",
    "\n",
    "# Directories\n",
    "parent_directory = \"/path/to/your/folder/\" # Replace with the actual path\n",
    "cellpose_directory = \"/path/to/your/folder/\" # Replace with the actual path\n",
    "model_name = \"MyoFuse.pth\" # Replace with the name of your classifier\n",
    "\n",
    "# Pre-processing\n",
    "myotube_channel = 1 # Replace with the actual myotube channel in your images\n",
    "nuclei_channel = 0 # Replace with the actual nuclei channel in your images\n",
    "extension = (\".tif\", \".tiff\")\n",
    "\n",
    "# Segmentation parameters\n",
    "dia = 24 # Adjust this value depending on your images. \n",
    "         # Set it to 0 for Cellpose to automatically determine the best value.\n",
    "\n",
    "# Classification parameters\n",
    "half_patch_size = 100\n",
    "batch_size = 512\n",
    "\n",
    "# Prediction parameters\n",
    "save_prediction = True  # Set to False to disable saving of prediction images\n",
    "downsampling_factor = 1 # Downsampling factor for prediction image. 1 : Full resolution.\n",
    "\n",
    "# Define paths\n",
    "full_images_folder = os.path.join(parent_directory, \"Full Images\")\n",
    "myotube_folder = os.path.join(parent_directory, \"Myotubes\")\n",
    "nuclei_folder = os.path.join(parent_directory, \"Nuclei\")\n",
    "masks_folder = os.path.join(parent_directory, \"Masks\")\n",
    "norm_folder = os.path.join(parent_directory, \"Images\")\n",
    "predictions_output_dir = os.path.join(parent_directory, \"Predictions\")\n",
    "model_path = os.path.join(parent_directory, 'Svetlana', model_name)\n",
    "config_path = os.path.join(parent_directory, \"Svetlana\", \"Config.json\")\n",
    "\n",
    "# Ensure output folders exists\n",
    "os.makedirs(myotube_folder, exist_ok=True)\n",
    "os.makedirs(nuclei_folder, exist_ok=True)\n",
    "os.makedirs(masks_folder, exist_ok=True)\n",
    "os.makedirs(norm_folder, exist_ok=True)\n",
    "os.makedirs(predictions_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f345efc-2d51-48e8-b21f-f68d86caf0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMAGE SPLITTING ## \n",
    "'''\n",
    "IF NECESSARY - Split images into separate channels for processing\n",
    "'''\n",
    "## Process each image in \"Full Images\"\n",
    "if os.path.exists(full_images_folder):\n",
    "    for file in os.listdir(full_images_folder):\n",
    "        file_path = os.path.join(full_images_folder, file)\n",
    "\n",
    "        # Check if it's an image file\n",
    "        if file.lower().endswith((\".tiff\", \".tif\")):\n",
    "            image = imread(file_path)\n",
    "\n",
    "        elif file.lower().endswith((\".ome.tiff\", \".czi\")):\n",
    "            # Désactiver la reconstruction de la mosaïque pour éviter le problème de broadcasting\n",
    "            aics_image = AICSImage(file_path, reconstruct_mosaic=False)\n",
    "            image = aics_image.data[0]  # Récupérer les données pour la première scène\n",
    "            print(image.shape)\n",
    "            if image.ndim == 5:  # TCZYX\n",
    "                image = image[0, 0].transpose((1, 2, 0))\n",
    "            elif image.ndim == 4:  # CZYX\n",
    "                image = image[0].transpose((1, 2, 0))\n",
    "            else:\n",
    "                print(f\"Skipped {file}: unsupported file format.\")\n",
    "                continue\n",
    "\n",
    "        # Check if the image has at least 2 channels\n",
    "        if image.ndim == 3 and image.shape[2] >= 2:\n",
    "            myotube = image[myotube_channel, :, :]\n",
    "            nuclei = image[nuclei_channel, :, :]\n",
    "\n",
    "            # Save Myotube channel\n",
    "            myotube_save_path = os.path.join(myotube_folder, f\"{os.path.splitext(file)[0]}.tif\")\n",
    "            imsave(myotube_save_path, myotube.astype(np.uint16))\n",
    "\n",
    "            # Save Nuclei channel\n",
    "            nuclei_save_path = os.path.join(nuclei_folder, f\"{os.path.splitext(file)[0]}.tif\")\n",
    "            imsave(nuclei_save_path, nuclei.astype(np.uint16))\n",
    "\n",
    "            print(f\"Processed and saved channels for: {file}\")\n",
    "        else:\n",
    "            print(f\"Skipped {file}: not enough channels.\")\n",
    "else:\n",
    "    print(f\"The folder 'Full Images' does not exist in {parent_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf2dcc-fa2d-49d4-94db-251d0032ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SEGMENTATION ##\n",
    "\n",
    "# List to store all image file paths\n",
    "image_files = []\n",
    "\n",
    "# Check if the \"Nuclei\" folder exists\n",
    "if os.path.exists(nuclei_folder):\n",
    "    for file in os.listdir(nuclei_folder):\n",
    "        file_path = os.path.join(nuclei_folder, file)\n",
    "        # Check if the file is an image\n",
    "        if file.lower().endswith(( \".tiff\", \".tif\")):\n",
    "            image_files.append(file_path)\n",
    "else:\n",
    "    print(f\"The folder 'Nuclei' does not exist in {parent_directory}\")\n",
    "\n",
    "# Print the list of images\n",
    "for index, file in enumerate(image_files):\n",
    "    print(f\"{file}\")\n",
    "\n",
    "    def load_image(file_path):\n",
    "    \n",
    "        # Load a single channel image\n",
    "        image = AICSImage(file_path).data[0][0] \n",
    "        return image\n",
    "\n",
    "    def loop(file_path, parent_directory, diameter):\n",
    "        base_name = os.path.basename(file_path)\n",
    "        name_without_ext = os.path.splitext(base_name)[0]\n",
    "        \n",
    "        # Load image\n",
    "        img = load_image(file_path)\n",
    "    \n",
    "        # Cellpose\n",
    "        model = models.CellposeModel(\n",
    "            gpu=False,\n",
    "            pretrained_model= cellpose_directory)\n",
    "        masks, flows, styles = model.eval(img, diameter, channels=[0, 0], normalize=True)\n",
    "    \n",
    "         # Save mask as .tiff\n",
    "        if masks.max() > 0:\n",
    "            mask_save_path = os.path.join(masks_folder, name_without_ext + \".tif\")\n",
    "            imwrite(mask_save_path, masks.astype(np.uint32), compression='zlib')\n",
    "            print(f\"Mask saved: {mask_save_path}\")\n",
    "        else:\n",
    "            print(f\"No cells detected in {file_path}, no mask has been saved.\")\n",
    "    \n",
    "        result = {\n",
    "            'image': file_path,\n",
    "            'diameter': diameter,\n",
    "            'cells count': np.max(masks)\n",
    "        }\n",
    "\n",
    "        # Empty GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        return result\n",
    "\n",
    "# List all image files in the \"Nuclei\" folder\n",
    "image_files = [f for f in os.listdir(nuclei_folder) if f.lower().endswith(('.tif', '.tiff'))]\n",
    "\n",
    "# Process each image\n",
    "for file in image_files:\n",
    "    file_path = os.path.join(nuclei_folder, file)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    result = loop(file_path, parent_directory, dia)                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01d8b8-d950-4a96-af13-10e910855786",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASSIFICATION PRE-PROCESSING ##\n",
    "\n",
    "## Normalize image\n",
    "\n",
    "def normalize_image_first_percentile(image, percentile_min, percentile_max):\n",
    "    \n",
    "    # Extract non-zero pixels\n",
    "    nonzero_pixels = image[image > 0]\n",
    "    if nonzero_pixels.size == 0:\n",
    "        return image  # Nothing to normalize if the image only contains zeros\n",
    "\n",
    "    # Define the new minimum based on the chosen percentile among non-zero pixels\n",
    "    new_min = np.percentile(nonzero_pixels, percentile_min)\n",
    "    max_val = np.percentile(nonzero_pixels, percentile_max)\n",
    "\n",
    "    # Create a normalized image initialized to 0 (to keep pixels at 0 unchanged)\n",
    "    norm_image = np.zeros_like(image)\n",
    "\n",
    "    # Apply normalization only for pixels greater than or equal to new_min\n",
    "    image[image < new_min] = 0\n",
    "    mask = image >= new_min\n",
    "    norm_image[mask] = (image[mask] - new_min) / (max_val - new_min)\n",
    "\n",
    "    # Clip to ensure values stay within [0, 1]\n",
    "    norm_image = np.clip(norm_image, 0, 1)\n",
    "\n",
    "    return norm_image\n",
    "\n",
    "def save_labels_images(\n",
    "    labels, \n",
    "    myotube_image, \n",
    "    output_path,\n",
    "    percentile=0\n",
    "):\n",
    "    if myotube_image.ndim == 3 and myotube_image.shape[2] == 3:\n",
    "        myotube_image = np.mean(myotube_image, axis=2)\n",
    "    \n",
    "    # Ensure image is kept at full resolution and original dtype\n",
    "    myotube_image = myotube_image.astype(np.float32)  # Preserve precision\n",
    "\n",
    "    # Apply percentile-based normalization\n",
    "    percentile_min = 4\n",
    "    percentile_max = 99.7\n",
    "    myotube_image = normalize_image_first_percentile(myotube_image, percentile_min, percentile_max)\n",
    "\n",
    "    # Save the normalized image\n",
    "    imsave(output_path, myotube_image, check_contrast=False)  # Preserve quality\n",
    "\n",
    "    print(f\"Processed {output_path}\")\n",
    "\n",
    "def process_folder(myotube_folder, labels_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    myotube_files = sorted(f for f in os.listdir(myotube_folder) if f.lower().endswith(('.tif', '.tiff')))\n",
    "    \n",
    "    for file in myotube_files:\n",
    "        myotube_path = os.path.join(myotube_folder, file)\n",
    "        label_path = os.path.join(labels_folder, file)\n",
    "        output_path = os.path.join(output_folder, file)\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Label missing for {file}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        myotube_image = imread(myotube_path)\n",
    "        labels = imread(label_path)\n",
    "        \n",
    "        save_labels_images(labels, myotube_image, output_path)\n",
    "        \n",
    "process_folder(myotube_folder, masks_folder, norm_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e9039-6b36-4f46-8b13-e4cc5244a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASSIFICATION ##\n",
    "\n",
    "## Collect label centroids\n",
    "\n",
    "def max_to_one(im):\n",
    "    \n",
    "    im = im / im.max()\n",
    "\n",
    "    return im\n",
    "\n",
    "def compute_label_centroids(label_image):\n",
    "    \n",
    "    label_image = label_image.astype(np.int32, copy=False)\n",
    "    labels_unique = np.unique(label_image)\n",
    "    labels_unique = labels_unique[labels_unique != 0]\n",
    "\n",
    "    weights = np.ones_like(label_image, dtype=np.float32)\n",
    "    cm = ndimage.center_of_mass(weights, labels=label_image, index=labels_unique)\n",
    "\n",
    "    return labels_unique, cm\n",
    "\n",
    "## Load images, masks\n",
    "\n",
    "def load_images_labels_centroids(parent_directory):\n",
    "\n",
    "    image_files = sorted([f for f in os.listdir(norm_folder) if f.lower().endswith(('.tif', '.tiff'))])\n",
    "    \n",
    "    images, labels, all_label_ids, all_centroids = [], [], [], []\n",
    "\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(norm_folder, img_file)\n",
    "        mask_path = os.path.join(masks_folder, img_file)\n",
    "\n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"No corresponding mask for {img_file}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load Image \n",
    "        image_data = imread(img_path).astype(np.float32)\n",
    "        \n",
    "        if image_data.ndim == 2:\n",
    "            # Duplique en 3 canaux\n",
    "            image_data = np.stack([image_data]*3, axis=-1)\n",
    "\n",
    "        # Convert in float32\n",
    "        image_data = image_data.astype(np.float32, copy=False)\n",
    "\n",
    "        # Load Mask\n",
    "        mask_data = imread(mask_path)\n",
    "\n",
    "        # Calculate centroids\n",
    "        label_ids, centroids = compute_label_centroids(mask_data)\n",
    "\n",
    "        # Store\n",
    "        images.append(image_data)\n",
    "        labels.append(mask_data)\n",
    "        all_label_ids.append(label_ids)\n",
    "        all_centroids.append(centroids)\n",
    "\n",
    "    return image_files, images, labels, all_label_ids, all_centroids\n",
    "\n",
    "## Prediction functions\n",
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    \"\"\"\n",
    "      prop.centroid = (row, col)  =>  cx = row, cy = col  \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image, labels, label_ids, centroids, half_patch_size, device, config_dict):\n",
    "        super().__init__()\n",
    "        self.image = image\n",
    "        self.labels = labels\n",
    "        self.label_ids = label_ids\n",
    "        self.centroids = centroids\n",
    "        self.half_patch_size = half_patch_size\n",
    "        self.device = device\n",
    "        self.config_dict = config_dict\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        pad = half_patch_size + 1\n",
    "\n",
    "        self.image = np.pad(\n",
    "            self.image,\n",
    "            ((pad, pad), (pad, pad), (0, 0)),\n",
    "            mode=\"constant\"\n",
    "        )\n",
    "        if self.labels.dtype != np.int32:\n",
    "            self.labels = self.labels.astype(np.int32, copy=False)\n",
    "        self.labels = np.pad(\n",
    "            self.labels,\n",
    "            ((pad, pad), (pad, pad)),\n",
    "            mode=\"constant\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "           \n",
    "            row, col = self.centroids[idx] \n",
    "            cx = int(row)\n",
    "            cy = int(col)\n",
    "            cx += self.half_patch_size + 1\n",
    "            cy += self.half_patch_size + 1\n",
    "            hps = self.half_patch_size\n",
    "            xmin, xmax = cx - hps, cx + hps\n",
    "            ymin, ymax = cy - hps, cy + hps\n",
    "\n",
    "            # Extract patch\n",
    "            patch_img = self.image[xmin:xmax, ymin:ymax, :].copy()\n",
    "            patch_mask = self.labels[xmin:xmax, ymin:ymax].copy()\n",
    "\n",
    "            label_id = self.label_ids[idx]\n",
    "\n",
    "            # Normalize\n",
    "            patch_img = max_to_one(patch_img)\n",
    "\n",
    "            # Binarize\n",
    "            patch_mask[patch_mask != label_id] = 0\n",
    "            patch_mask[patch_mask == label_id] = 1\n",
    "\n",
    "            # Dilation if stated in the config file\n",
    "            do_dilate = json.loads(self.config_dict[\"options\"][\"dilation\"][\"dilate_mask\"].lower())\n",
    "            if do_dilate:\n",
    "                se_size = int(self.config_dict[\"options\"][\"dilation\"][\"str_element_size\"])\n",
    "                strel = disk(se_size)\n",
    "                patch_mask = dilation(patch_mask, strel)\n",
    "                patch_img *= patch_mask[..., None]\n",
    "\n",
    "            # Mask concatenation as 4th channel\n",
    "            out = np.zeros((patch_img.shape[0], patch_img.shape[1], 4), dtype=np.float32)\n",
    "            out[..., :3] = patch_img\n",
    "            out[..., 3] = patch_mask\n",
    "\n",
    "            # Transformation to (C, H, W) + Tensor\n",
    "            out = self.transform(out)\n",
    "            return out.to(self.device)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Dataset] Erreur index {idx} : {e}\")\n",
    "            return None\n",
    "\n",
    "## Save prediction image\n",
    "\n",
    "def save_colored_predictions_downsample(\n",
    "    labels, \n",
    "    predictions, \n",
    "    used_labels, \n",
    "    myotube_image, \n",
    "    output_path, \n",
    "    factor=1\n",
    "):\n",
    "    if myotube_image.ndim == 3 and myotube_image.shape[2] == 3:\n",
    "        myotube_image = np.mean(myotube_image, axis=2)\n",
    "    H, W = myotube_image.shape\n",
    "\n",
    "    newH, newW = H//factor, W//factor\n",
    "    myotube_ds = resize(myotube_image, (newH, newW),\n",
    "                        preserve_range=True,\n",
    "                        anti_aliasing=True)\n",
    "    \n",
    "    # Removing lowest and highest values\n",
    "    p2, p98 = np.percentile(myotube_ds, (2, 98))\n",
    "\n",
    "    myotube_ds = exposure.rescale_intensity(myotube_ds, in_range=(p2, p98), out_range=(0,1))\n",
    "\n",
    "    myotube_rgb = np.stack([myotube_ds]*3, axis=-1)\n",
    "\n",
    "    boundaries = find_boundaries(labels, mode='inner')\n",
    "    boundary_labels = labels.copy()\n",
    "    boundary_labels[~boundaries] = 0\n",
    "\n",
    "    if len(predictions) != len(used_labels):\n",
    "        raise ValueError(\"Nombre de prédictions != nombre de labels.\")\n",
    "    label_to_pred = dict(zip(used_labels, predictions))\n",
    "\n",
    "    color_0 = [1.0, 0.0, 0.0]  # Rouge = Nuclei Out\n",
    "    color_1 = [0.0, 1.0, 0.0]  # Vert = Nuclei In\n",
    "\n",
    "    coords = np.column_stack(np.nonzero(boundary_labels))\n",
    "    for (y, x) in coords:\n",
    "        lb = boundary_labels[y, x]\n",
    "        pred_class = label_to_pred.get(lb, None)\n",
    "        if pred_class is not None:\n",
    "            yd = y // factor\n",
    "            xd = x // factor\n",
    "            if yd < newH and xd < newW:\n",
    "                if pred_class == 0:\n",
    "                    myotube_rgb[yd, xd] = color_0\n",
    "                else:\n",
    "                    myotube_rgb[yd, xd] = color_1\n",
    "\n",
    "    from skimage.io import imsave\n",
    "    myotube_rgb_8 = img_as_ubyte(myotube_rgb)\n",
    "    imsave(output_path, myotube_rgb_8)\n",
    "\n",
    "## Main loop for prediction\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Load config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "\n",
    "    # Load classifier\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print (\"Classification : \", device)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = checkpoint[\"model\"].to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load Images + Masks + Centroids\n",
    "    image_files, images, labels, all_label_ids, all_centroids = load_images_labels_centroids(parent_directory)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for i, img_file in enumerate(image_files):\n",
    "        current_image = images[i]\n",
    "        current_label = labels[i]\n",
    "        label_ids = all_label_ids[i]\n",
    "        centroids = all_centroids[i]  # liste (row, col)\n",
    "\n",
    "        dataset = PredictionDataset(\n",
    "            image=current_image,\n",
    "            labels=current_label,\n",
    "            label_ids=label_ids,\n",
    "            centroids=centroids,\n",
    "            half_patch_size=half_patch_size,\n",
    "            device=device,\n",
    "            config_dict=config_dict\n",
    "        )\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        image_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch_tensor in dataloader:\n",
    "                if batch_tensor is None or batch_tensor.size(0) == 0:\n",
    "                    continue\n",
    "                outputs = model(batch_tensor)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                image_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "        # Save prediction image\n",
    "        if save_prediction == True :\n",
    "            image_name = os.path.splitext(img_file)[0]\n",
    "            output_path = os.path.join(predictions_output_dir, f\"{image_name}_prediction.tif\")\n",
    "            save_colored_predictions_downsample(\n",
    "                labels=current_label,\n",
    "                predictions=image_preds,\n",
    "                used_labels=label_ids,\n",
    "                myotube_image=current_image,\n",
    "                output_path=output_path,\n",
    "                factor = downsampling_factor\n",
    "            )\n",
    "        \n",
    "        all_predictions.append(image_preds)\n",
    "\n",
    "    ## Export results\n",
    "    \n",
    "    # List to store statistics for each image\n",
    "    prediction_data = []\n",
    "\n",
    "    for i, predictions in enumerate(all_predictions):\n",
    "        pred_array = np.array(predictions)\n",
    "        \n",
    "        # Count 0 / 1\n",
    "        num_ones = np.sum(pred_array == 1)\n",
    "        num_zeros = np.sum(pred_array == 0)\n",
    "        total_labels = len(pred_array)\n",
    "\n",
    "        if total_labels > 0:\n",
    "            fusion_index = (num_ones / total_labels) * 100.0\n",
    "        else:\n",
    "            fusion_index = 0.0\n",
    "        \n",
    "        image_name = os.path.splitext(os.path.basename(image_files[i]))[0]\n",
    "        \n",
    "        prediction_data.append({\n",
    "            \"Image Name\": image_name,\n",
    "            \"Total Number of Nuclei\": total_labels,\n",
    "            \"Nuclei In\": num_ones,\n",
    "            \"Nuclei Out\": num_zeros,\n",
    "            \"Fusion Index (%)\": fusion_index\n",
    "        })\n",
    "\n",
    "    predictions_df = pd.DataFrame(prediction_data)\n",
    "    output_file_path = os.path.join(parent_directory, \"Fusion_index.xlsx\")\n",
    "    predictions_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "    print(f\"\\nDataFrame exported to Excel file: {output_file_path}\")\n",
    "    print(\"Classification Done\")\n",
    "\n",
    "    return all_predictions, predictions_df\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
